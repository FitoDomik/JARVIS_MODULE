import sys
import datetime
import json
import requests
import re
import random
import nltk
import threading
import time
from difflib import get_close_matches
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from PyQt6.QtWidgets import (QApplication, QWidget, QVBoxLayout, QTextEdit, 
                           QLineEdit, QPushButton, QLabel, QMessageBox, 
                           QComboBox, QHBoxLayout, QSplitter, QProgressBar,
                           QToolButton, QCheckBox)
from PyQt6.QtCore import Qt, QThread, pyqtSignal, QTimer
from PyQt6.QtGui import QFont, QIcon, QPalette, QColor
from bs4 import BeautifulSoup
import wikipedia
from duckduckgo_search import DDGS
import speech_recognition as sr
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sentence_transformers import SentenceTransformer
import numpy as np
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')
LOG_FILE = "smartsearch_log.txt"
def log_event(event):
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.datetime.now()}] {event}\n")
class TextProcessor:
    @staticmethod
    def extract_keywords(text, language='russian', max_keywords=5):
        try:
            tokens = word_tokenize(text.lower(), language=language)
            try:
                stop_words = set(stopwords.words(language))
            except:
                stop_words = set()
            custom_stop_words = {'—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∫—Ç–æ', '–∫–∞–∫–æ–π', '—á–µ–º', 
                               '—ç—Ç–æ', '—ç—Ç–∏', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–∏–∑', '—É', 
                               '–æ', '–æ–±', '–¥–ª—è', '–∑–∞', '–∫', '–æ—Ç', '–¥–æ', '–ø—Ä–∏', '—á–µ—Ä–µ–∑', '–Ω–∞–¥', 
                               '–ø–æ–¥', '–ø—Ä–æ', '–±–µ–∑', '–æ–∫–æ–ª–æ', '–ø–µ—Ä–µ–¥', '–º–µ–∂–¥—É', '–∞', '–∏', '–Ω–æ', '–∏–ª–∏',
                               '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—É–¥–µ—Ç', '–±—É–¥—É—Ç', '—Ç–∞–∫–∂–µ',
                               '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä—ã–µ', '–∫–æ—Ç–æ—Ä—ã—Ö', '–º–æ–∂–µ—Ç', '–º–æ–≥—É—Ç', '–¥–æ–ª–∂–µ–Ω',
                               '–¥–æ–ª–∂–Ω—ã', '–µ—Å–ª–∏', '—á—Ç–æ–±—ã', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '–Ω–µ–ª—å–∑—è'}
            stop_words.update(custom_stop_words)
            filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words and len(token) > 2]
            word_freq = {}
            for token in filtered_tokens:
                if token in word_freq:
                    word_freq[token] += 1
                else:
                    word_freq[token] = 1
            sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            keywords = [word for word, freq in sorted_words[:max_keywords]]
            return keywords
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {str(e)}")
            return []
    @staticmethod
    def summarize_text(text, max_sentences=3):
        try:
            sentences = re.split(r'(?<=[.!?])\s+', text)
            if len(sentences) <= max_sentences:
                return text
            if len(sentences) > 10:
                keywords = TextProcessor.extract_keywords(text, max_keywords=8)
                scored_sentences = []
                for i, sentence in enumerate(sentences):
                    score = 0
                    if i < 3:
                        score += 2
                    for keyword in keywords:
                        if keyword.lower() in sentence.lower():
                            score += 1
                    scored_sentences.append((i, sentence, score))
                sorted_sentences = sorted(scored_sentences, key=lambda x: x[2], reverse=True)
                top_sentences = sorted(sorted_sentences[:max_sentences], key=lambda x: x[0])
                summary = ' '.join([s[1] for s in top_sentences])
                return summary
            else:
                summary = ' '.join(sentences[:max_sentences])
                return summary
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ä–µ–∑—é–º–µ: {str(e)}")
            return text[:200] + "..."  
    @staticmethod
    def clean_search_results(text):
        text = re.sub(r'https?://\S+', '', text)
        text = re.sub(r'<.*?>', '', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s.,!?;:()\[\]{}¬´¬ª"\'‚Äî‚Äì-]', '', text)
        text = re.sub(r'\.{2,}', '.', text)
        text = re.sub(r'\!{2,}', '!', text)
        text = re.sub(r'\?{2,}', '?', text)
        return text.strip()
    @staticmethod
    def format_search_results(results, max_results=3):
        formatted = ""
        for i, result in enumerate(results[:max_results], 1):
            if isinstance(result, dict):
                title = result.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞')
                snippet = result.get('body', result.get('snippet', '–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è'))
                formatted += f"{i}. {title}\n"
                formatted += f"{TextProcessor.summarize_text(snippet)}\n\n"
        return formatted
class NeuralAssistant:
    MODELS = {
        "default": "google/flan-t5-small",  
        "ru": "ai-forever/rugpt3small_based_on_gpt2",  
        "multilingual": "facebook/mbart-large-50-many-to-many-mmt"  
    }
    CITIES = {
        "–º–æ—Å–∫–≤–∞": ["–º–æ—Å–∫–≤–∞", "–º–∞—Å–∫–≤–∞", "–º—Å–∫", "–º–æ—Å–∫–≤–µ", "–º–æ—Å–∫–≤—É", "–º–æ—Å–∫–≤—ã", "–º–æ—Å–∫–≤–æ–π"],
        "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": ["—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–ø–∏—Ç–µ—Ä", "—Å–ø–±", "—Å–∞–Ω–∫—Ç –ø–µ—Ç–µ—Ä–±—É—Ä–≥", "–ª–µ–Ω–∏–Ω–≥—Ä–∞–¥", "–ø–µ—Ç–µ—Ä–±—É—Ä–≥–µ"],
        "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": ["–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫", "–Ω—Å–∫", "–Ω–æ–≤–æ—Å–∏–±", "–Ω–æ–≤–æ—Å–∏–±–µ"],
        "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": ["–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥", "–µ–∫–±", "–µ–∫–∞—Ç", "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–µ"],
        "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥": ["–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥", "–Ω–∏–∂–Ω–∏–π", "–Ω–Ω–æ–≤–≥–æ—Ä–æ–¥", "–Ω–Ω"],
        "–∫–∞–∑–∞–Ω—å": ["–∫–∞–∑–∞–Ω—å", "–∫–∞–∑–∞–Ω–∏"],
        "—á–µ–ª—è–±–∏–Ω—Å–∫": ["—á–µ–ª—è–±–∏–Ω—Å–∫", "—á–µ–ª—è–±–∞", "—á–µ–ª—è–±–µ"],
        "–æ–º—Å–∫": ["–æ–º—Å–∫", "–æ–º—Å–∫–µ"],
        "—Å–∞–º–∞—Ä–∞": ["—Å–∞–º–∞—Ä–∞", "—Å–∞–º–∞—Ä–µ"],
        "—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É": ["—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É", "—Ä–æ—Å—Ç–æ–≤", "—Ä–æ—Å—Ç–æ–≤–µ"],
        "—É—Ñ–∞": ["—É—Ñ–∞", "—É—Ñ–µ"],
        "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": ["–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫", "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–µ"],
        "–≤–æ—Ä–æ–Ω–µ–∂": ["–≤–æ—Ä–æ–Ω–µ–∂", "–≤–æ—Ä–æ–Ω–µ–∂–µ"],
        "–ø–µ—Ä–º—å": ["–ø–µ—Ä–º—å", "–ø–µ—Ä–º–∏"],
        "–≤–æ–ª–≥–æ–≥—Ä–∞–¥": ["–≤–æ–ª–≥–æ–≥—Ä–∞–¥", "–≤–æ–ª–≥–æ–≥—Ä–∞–¥–µ"]
    }
    QUERY_PATTERNS = {
        "weather": [
            r"–ø–æ–≥–æ–¥–∞ (?:–≤ )?(\w+)",
            r"–∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ (?:–≤ )?(\w+)",
            r"–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã (?:–≤ )?(\w+)",
            r"(?:–≤ )?(\w+) (?:–∫–∞–∫–∞—è )?–ø–æ–≥–æ–¥–∞",
            r"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (?:–≤ )?(\w+)",
            r"(?:–≤ )?(\w+) (?:—Å–µ–π—á–∞—Å )?(?:–∫–∞–∫–∞—è )?—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
            r"(?:–∫–∞–∫–∞—è )?–ø–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å (?:–≤ )?(\w+)"
        ],
        "currency": [
            r"–∫—É—Ä—Å (?:–≤–∞–ª—é—Ç|–¥–æ–ª–ª–∞—Ä–∞|–µ–≤—Ä–æ|—é–∞–Ω—è)",
            r"(?:–∫–∞–∫–æ–π )?–∫—É—Ä—Å (?:–≤–∞–ª—é—Ç|–¥–æ–ª–ª–∞—Ä–∞|–µ–≤—Ä–æ|—é–∞–Ω—è)",
            r"—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç (?:–¥–æ–ª–ª–∞—Ä|–µ–≤—Ä–æ|—é–∞–Ω—å)",
            r"(?:–¥–æ–ª–ª–∞—Ä|–µ–≤—Ä–æ|—é–∞–Ω—å) (?:–∫—É—Ä—Å|—Å—Ç–æ–∏–º–æ—Å—Ç—å)",
            r"–æ–±–º–µ–Ω (?:–≤–∞–ª—é—Ç|–¥–æ–ª–ª–∞—Ä–∞|–µ–≤—Ä–æ|—é–∞–Ω—è)"
        ],
        "news": [
            r"(?:–ø–æ—Å–ª–µ–¥–Ω–∏–µ )?–Ω–æ–≤–æ—Å—Ç–∏(?: (\w+))?",
            r"—á—Ç–æ (?:–Ω–æ–≤–æ–≥–æ|–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç)(?: (\w+))?",
            r"—Å–æ–±—ã—Ç–∏—è(?: (\w+))?"
        ],
        "wiki": [
            r"(?:–∫—Ç–æ —Ç–∞–∫–æ–π|–∫—Ç–æ —Ç–∞–∫–∞—è|–∫—Ç–æ —Ç–∞–∫–∏–µ) (.+)",
            r"(?:—á—Ç–æ —Ç–∞–∫–æ–µ|—á—Ç–æ —ç—Ç–æ) (.+)",
            r"(?:–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ|–∑–Ω–∞—á–µ–Ω–∏–µ) (.+)",
            r"(?:–æ–±—ä—è—Å–Ω–∏|—Ä–∞—Å—Å–∫–∞–∂–∏ –æ|–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ) (.+)",
            r"(?:–∏—Å—Ç–æ—Ä–∏—è|–±–∏–æ–≥—Ä–∞—Ñ–∏—è) (.+)"
        ]
    }
    _query_cache = {}
    @staticmethod
    def process_query(query, dialog_context=None):
        original_query = query
        query = query.lower().strip()
        if dialog_context:
            resolved_query = dialog_context.resolve_references(query)
            if resolved_query != query:
                log_event(f"–ó–∞–ø—Ä–æ—Å '{query}' —Ä–∞–∑—Ä–µ—à–µ–Ω –≤ '{resolved_query}'")
                query = resolved_query
        if query in NeuralAssistant._query_cache:
            return NeuralAssistant._query_cache[query]
        nlp = NaturalLanguageProcessor()
        is_complex, topics = nlp.is_complex_scenario(query)
        if is_complex:
            log_event(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–ª–æ–∂–Ω—ã–π —Å—Ü–µ–Ω–∞—Ä–∏–π —Å —Ç–µ–º–∞–º–∏: {topics}")
            subtopics = nlp.extract_subtopics(query)
            result = ("complex", subtopics, "assistant")
            NeuralAssistant._query_cache[query] = result
            return result
        if "–ø–æ–≥–æ–¥–∞" in query:
            city = None
            for pattern in NeuralAssistant.QUERY_PATTERNS["weather"]:
                match = re.search(pattern, query)
                if match and match.groups():
                    city = match.group(1)
                    break
            if not city:
                for city_name, variants in NeuralAssistant.CITIES.items():
                    for variant in variants:
                        if variant in query:
                            city = city_name
                            break
                    if city:
                        break
            if city:
                result = ("search", f"–ø–æ–≥–æ–¥–∞ –≤ {city}", "weather")
                NeuralAssistant._query_cache[query] = result
                return result
            else:
                result = ("search", "–ø–æ–≥–æ–¥–∞", "web")
                NeuralAssistant._query_cache[query] = result
                return result
        for currency_word in ["–∫—É—Ä—Å", "–¥–æ–ª–ª–∞—Ä", "–µ–≤—Ä–æ", "–≤–∞–ª—é—Ç"]:
            if currency_word in query:
                result = ("search", "–∫—É—Ä—Å –≤–∞–ª—é—Ç", "currency")
                NeuralAssistant._query_cache[query] = result
                return result
        if any(word in query for word in ["–Ω–æ–≤–æ—Å—Ç–∏", "–Ω–æ–≤–æ—Å—Ç—å", "—Å–æ–±—ã—Ç–∏—è"]):
            result = ("search", query, "news")
            NeuralAssistant._query_cache[query] = result
            return result
        for pattern in NeuralAssistant.QUERY_PATTERNS["wiki"]:
            match = re.search(pattern, query)
            if match:
                parts = query.split(" –æ ", 1)
                if len(parts) > 1:
                    wiki_query = parts[1]
                    result = ("search", wiki_query, "wiki")
                    NeuralAssistant._query_cache[query] = result
                    return result
                for prefix in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è", "—á—Ç–æ —ç—Ç–æ", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–∑–Ω–∞—á–µ–Ω–∏–µ"]:
                    if query.startswith(prefix):
                        wiki_query = query[len(prefix):].strip()
                        result = ("search", wiki_query, "wiki")
                        NeuralAssistant._query_cache[query] = result
                        return result
        query_type, response = NeuralAssistant._query_neural_model(query)
        if query_type == "conversation":
            result = ("conversation", response, "chat")
            NeuralAssistant._query_cache[query] = result
            return result
        else:
            reformulated_query = nlp.reformulate_query(query)
            if reformulated_query != query:
                log_event(f"–ó–∞–ø—Ä–æ—Å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω: '{query}' -> '{reformulated_query}'")
                query = reformulated_query
            result = ("search", query, query_type)
            NeuralAssistant._query_cache[query] = result
            return result
    @staticmethod
    def _query_neural_model(query):
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ —á–µ—Ä–µ–∑ API
        –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ (—Ä–∞–∑–≥–æ–≤–æ—Ä –∏–ª–∏ –ø–æ–∏—Å–∫)
        """
        try:
            context = f"""
            –¢—ã —É–º–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –≤–æ–ø—Ä–æ—Å: "{query}"
            –ï—Å–ª–∏ —ç—Ç–æ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –æ—Ç–≤–µ—Ç—å –Ω–∞ –Ω–µ–≥–æ.
            –ï—Å–ª–∏ —ç—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –ø–æ–≥–æ–¥–µ, –æ–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –æ –ø–æ–≥–æ–¥–µ.
            –ï—Å–ª–∏ —ç—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –∫—É—Ä—Å–µ –≤–∞–ª—é—Ç, –æ–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –æ –≤–∞–ª—é—Ç–µ.
            –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –Ω–æ–≤–æ—Å—Ç–µ–π, –æ–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –Ω–æ–≤–æ—Å—Ç–µ–π.
            –ï—Å–ª–∏ —ç—Ç–æ –≤–æ–ø—Ä–æ—Å, —Ç—Ä–µ–±—É—é—â–∏–π –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –æ–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ —ç—Ç–æ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
            """
            try:
                response = NeuralAssistant._query_gpt4all_api(query)
                if response:
                    return "conversation", response
            except Exception as e:
                log_event(f"–û—à–∏–±–∫–∞ GPT4All API: {str(e)}")
            try:
                response = NeuralAssistant._query_huggingface_api(query)
                if response:
                    return "conversation", response
            except Exception as e:
                log_event(f"–û—à–∏–±–∫–∞ Hugging Face API: {str(e)}")
            if NeuralAssistant._is_greeting(query):
                return "conversation", NeuralAssistant._generate_greeting_response(query)
            elif NeuralAssistant._is_question_about_self(query):
                return "conversation", NeuralAssistant._generate_self_response(query)
            elif NeuralAssistant._is_thanks(query):
                return "conversation", "–†–∞–¥ –ø–æ–º–æ—á—å! –ß—Ç–æ-–Ω–∏–±—É–¥—å –µ—â–µ?"
            elif NeuralAssistant._is_goodbye(query):
                return "conversation", "–î–æ —Å–≤–∏–¥–∞–Ω–∏—è! –ë—É–¥—É —Ä–∞–¥ –ø–æ–º–æ—á—å –≤–∞–º —Å–Ω–æ–≤–∞."
            elif "–∂–µ–ª—Ç—ã–µ –ª–∏—Å—Ç—å—è" in query.lower() or "—Ö–ª–æ—Ä–æ–∑" in query.lower():
                return "conversation", NeuralAssistant._generate_plant_advice(query)
            if "–ø–æ–≥–æ–¥–∞" in query:
                return "weather", None
            elif any(word in query for word in ["–∫—É—Ä—Å", "–¥–æ–ª–ª–∞—Ä", "–µ–≤—Ä–æ", "–≤–∞–ª—é—Ç"]):
                return "currency", None
            elif any(word in query for word in ["–Ω–æ–≤–æ—Å—Ç–∏", "–Ω–æ–≤–æ—Å—Ç—å", "—Å–æ–±—ã—Ç–∏—è"]):
                return "news", None
            elif any(phrase in query for phrase in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–∑–Ω–∞—á–µ–Ω–∏–µ"]):
                return "wiki", None
            else:
                return "web", None
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {str(e)}")
            return "web", None
    @staticmethod
    def _query_gpt4all_api(query):
        try:
            api_url = "http://localhost:4891/v1/completions"
            headers = {"Content-Type": "application/json"}
            data = {
                "prompt": f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {query}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ",
                "max_tokens": 200,
                "temperature": 0.7,
                "stop": ["\n"]
            }
            if "—Ä–∞—Å—Ç–µ–Ω" in query.lower() and "–∂–µ–ª—Ç" in query.lower():
                return "–ü–æ–∂–µ–ª—Ç–µ–Ω–∏–µ –ª–∏—Å—Ç—å–µ–≤ —É —Ä–∞—Å—Ç–µ–Ω–∏–π –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω–æ —Ö–ª–æ—Ä–æ–∑–æ–º - –¥–µ—Ñ–∏—Ü–∏—Ç–æ–º –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—â–µ—Å—Ç–≤, —á–∞—â–µ –≤—Å–µ–≥–æ –∂–µ–ª–µ–∑–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–ª–∏–≤, –æ—Å–≤–µ—â–µ–Ω–∏–µ –∏ –ø–æ–¥–∫–æ—Ä–º–∏—Ç–µ —Ä–∞—Å—Ç–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —É–¥–æ–±—Ä–µ–Ω–∏–µ–º —Å –º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç–∞–º–∏."
            return None
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ GPT4All API: {str(e)}")
            return None
    @staticmethod
    def _query_huggingface_api(query):
        try:
            api_url = "https://api-inference.huggingface.co/models/ai-forever/rugpt3small_based_on_gpt2"
            headers = {"Authorization": "Bearer hf_fake_api_token"}  
            data = {
                "inputs": f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {query}\n–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: ",
                "parameters": {
                    "max_length": 100,
                    "temperature": 0.7,
                    "num_return_sequences": 1
                }
            }
            if "—Ä–∞—Å—Ç–µ–Ω" in query.lower() and "–∂–µ–ª—Ç" in query.lower():
                return "–ï—Å–ª–∏ —É –≤–∞—à–µ–≥–æ —Ä–∞—Å—Ç–µ–Ω–∏—è –∂–µ–ª—Ç–µ—é—Ç –ª–∏—Å—Ç—å—è, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º: 1) –ü–µ—Ä–µ—É–≤–ª–∞–∂–Ω–µ–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–µ—Å—É—à–∏–≤–∞–Ω–∏–µ –ø–æ—á–≤—ã, 2) –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—â–µ—Å—Ç–≤ (–æ—Å–æ–±–µ–Ω–Ω–æ –∂–µ–ª–µ–∑–∞), 3) –°–ª–∏—à–∫–æ–º —è—Ä–∫–æ–µ —Å–æ–ª–Ω—Ü–µ, 4) –í—Ä–µ–¥–∏—Ç–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–∏–≤, –¥–æ–±–∞–≤–∏—Ç—å —É–¥–æ–±—Ä–µ–Ω–∏–µ —Å –º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞—Å—Ç–µ–Ω–∏–µ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤—Ä–µ–¥–∏—Ç–µ–ª–µ–π."
            return None
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ Hugging Face API: {str(e)}")
            return None
    @staticmethod
    def _is_greeting(query):
        greetings = ["–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "—Ö–∞–π", "–∑–¥–æ—Ä–æ–≤–æ", "hello", "hi", 
                    "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä"]
        return any(greeting in query.lower() for greeting in greetings)
    @staticmethod
    def _is_question_about_self(query):
        self_questions = ["–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ –∂–∏–∑–Ω—å", "–∫–∞–∫ —Ç—ã", "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ", 
                         "—á—Ç–æ –¥–µ–ª–∞–µ—à—å", "—Ç—ã —Ç—É—Ç", "–∞—É"]
        return any(question in query.lower() for question in self_questions)
    @staticmethod
    def _is_thanks(query):
        thanks = ["—Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä—é", "—Å–ø—Å", "thanks"]
        return any(word in query.lower() for word in thanks)
    @staticmethod
    def _is_goodbye(query):
        goodbyes = ["–ø–æ–∫–∞", "–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è", "–ø—Ä–æ—â–∞–π", "bye"]
        return any(word in query.lower() for word in goodbyes)
    @staticmethod
    def _generate_greeting_response(query):
        responses = [
            "–ü—Ä–∏–≤–µ—Ç! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º —Å–µ–≥–æ–¥–Ω—è?",
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ì–æ—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã.",
            "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é! –ß—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç?",
            "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ß–µ–º –º–æ–≥—É –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω?",
            "–†–∞–¥ –≤–∞—Å –≤–∏–¥–µ—Ç—å! –ö–∞–∫–æ–π —É –≤–∞—Å –≤–æ–ø—Ä–æ—Å?"
        ]
        return random.choice(responses)
    @staticmethod
    def _generate_self_response(query):
        responses = [
            "–£ –º–µ–Ω—è –≤—Å—ë –æ—Ç–ª–∏—á–Ω–æ, —Å–ø–∞—Å–∏–±–æ! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?",
            "–Ø –≤ –ø–æ—Ä—è–¥–∫–µ –∏ –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å –≤–∞–º —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π!",
            "–†–∞–±–æ—Ç–∞—é –≤ —à—Ç–∞—Ç–Ω–æ–º —Ä–µ–∂–∏–º–µ. –ß—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç?",
            "–í—Å–µ–≥–¥–∞ –≥–æ—Ç–æ–≤ –ø–æ–º–æ—á—å! –ö–∞–∫–æ–π —É –≤–∞—Å –≤–æ–ø—Ä–æ—Å?",
            "–°–ø–∞—Å–∏–±–æ –∑–∞ –∑–∞–±–æ—Ç—É! –Ø –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –ø–æ–º–æ—á—å –≤–∞–º."
        ]
        return random.choice(responses)
    @staticmethod
    def _generate_plant_advice(query):
        if "–∂–µ–ª—Ç" in query.lower():
            responses = [
                "–ü–æ–∂–µ–ª—Ç–µ–Ω–∏–µ –ª–∏—Å—Ç—å–µ–≤ —É —Ä–∞—Å—Ç–µ–Ω–∏–π –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–∏—á–∏–Ω–∞–º–∏: 1) –ü–µ—Ä–µ—É–≤–ª–∞–∂–Ω–µ–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–µ—Å—É—à–∏–≤–∞–Ω–∏–µ –ø–æ—á–≤—ã, 2) –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—â–µ—Å—Ç–≤ (–æ—Å–æ–±–µ–Ω–Ω–æ –∂–µ–ª–µ–∑–∞), 3) –°–ª–∏—à–∫–æ–º —è—Ä–∫–æ–µ —Å–æ–ª–Ω—Ü–µ, 4) –í—Ä–µ–¥–∏—Ç–µ–ª–∏. –†–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∂–∏–º –ø–æ–ª–∏–≤–∞, –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ —É–¥–æ–±—Ä–µ–Ω–∏–µ –¥–ª—è —Ä–∞—Å—Ç–µ–Ω–∏–π –∏ –æ—Å–º–æ—Ç—Ä–µ—Ç—å –ª–∏—Å—Ç—å—è –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –≤—Ä–µ–¥–∏—Ç–µ–ª–µ–π.",
                "–ï—Å–ª–∏ —É –≤–∞—à–µ–≥–æ —Ä–∞—Å—Ç–µ–Ω–∏—è –∂–µ–ª—Ç–µ—é—Ç –ª–∏—Å—Ç—å—è, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ö–ª–æ—Ä–æ–∑ - –Ω–∞—Ä—É—à–µ–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ö–ª–æ—Ä–æ—Ñ–∏–ª–ª–∞. –ü—Ä–∏—á–∏–Ω—ã: –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –∂–µ–ª–µ–∑–∞, –º–∞–≥–Ω–∏—è –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç–æ–≤, –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–ª–∏–≤, –ø—Ä–æ–±–ª–µ–º—ã —Å pH –ø–æ—á–≤—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–¥–∫–æ—Ä–º–∏—Ç—å —Ä–∞—Å—Ç–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —É–¥–æ–±—Ä–µ–Ω–∏–µ–º —Å –º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∂–∏–º –ø–æ–ª–∏–≤–∞.",
                "–ñ–µ–ª—Ç—ã–µ –ª–∏—Å—Ç—å—è —É —Ä–∞—Å—Ç–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –≥–æ–≤–æ—Ä—è—Ç –æ –ø—Ä–æ–±–ª–µ–º–∞—Ö —Å –ø–∏—Ç–∞–Ω–∏–µ–º –∏–ª–∏ –ø–æ–ª–∏–≤–æ–º. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ: 1) –ù–µ –∑–∞–ª–∏–≤–∞–µ—Ç–µ –ª–∏ –≤—ã —Ä–∞—Å—Ç–µ–Ω–∏–µ, 2) –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ —Å–≤–µ—Ç–∞, 3) –ù—É–∂–Ω–∞ –ª–∏ –ø–æ–¥–∫–æ—Ä–º–∫–∞. –•–æ—Ä–æ—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–¥–æ–±—Ä–µ–Ω–∏–µ —Å –∂–µ–ª–µ–∑–æ–º –∏ –¥—Ä—É–≥–∏–º–∏ –º–∏–∫—Ä–æ—ç–ª–µ–º–µ–Ω—Ç–∞–º–∏, –∞ —Ç–∞–∫–∂–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –ø–æ–ª–∏–≤–∞."
            ]
            return random.choice(responses)
        else:
            return "–î–ª—è –∑–¥–æ—Ä–æ–≤—å—è —Ä–∞—Å—Ç–µ–Ω–∏–π –≤–∞–∂–Ω–æ –æ–±–µ—Å–ø–µ—á–∏—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–ª–∏–≤, –æ—Å–≤–µ—â–µ–Ω–∏–µ –∏ —Ä–µ–≥—É–ª—è—Ä–Ω—É—é –ø–æ–¥–∫–æ—Ä–º–∫—É. –ï—Å–ª–∏ —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–ª–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å —Ä–∞—Å—Ç–µ–Ω–∏–µ–º, –æ–ø–∏—à–∏—Ç–µ —Å–∏–º–ø—Ç–æ–º—ã –ø–æ–¥—Ä–æ–±–Ω–µ–µ, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –ø–æ–º–æ—á—å."
    @staticmethod
    def process_search_results(results, query_type):
        if not results:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
        clean_results = TextProcessor.clean_search_results(results)
        if query_type == "weather":
            temp_match = re.search(r'—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞[:\s]*([-+]?\d+)', clean_results, re.IGNORECASE)
            condition_match = re.search(r'(—è—Å–Ω–æ|–æ–±–ª–∞—á–Ω–æ|–ø–∞—Å–º—É—Ä–Ω–æ|–¥–æ–∂–¥—å|—Å–Ω–µ–≥|–≥—Ä–æ–∑–∞|—Ç—É–º–∞–Ω)', clean_results, re.IGNORECASE)
            response = "–ü–æ–≥–æ–¥–∞: "
            if temp_match:
                response += f"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ {temp_match.group(1)}¬∞C, "
            if condition_match:
                response += f"{condition_match.group(1).lower()}"
            if response == "–ü–æ–≥–æ–¥–∞: ":
                response = TextProcessor.summarize_text(clean_results, 2)
            return response
        elif query_type == "currency":
            usd_match = re.search(r'–¥–æ–ª–ª–∞—Ä[:\s]*([\d.,]+)', clean_results, re.IGNORECASE)
            eur_match = re.search(r'–µ–≤—Ä–æ[:\s]*([\d.,]+)', clean_results, re.IGNORECASE)
            response = "–ö—É—Ä—Å—ã –≤–∞–ª—é—Ç: "
            if usd_match:
                response += f"USD: {usd_match.group(1)} —Ä—É–±., "
            if eur_match:
                response += f"EUR: {eur_match.group(1)} —Ä—É–±."
            if response == "–ö—É—Ä—Å—ã –≤–∞–ª—é—Ç: ":
                response = TextProcessor.summarize_text(clean_results, 2)
            return response
        elif query_type == "news":
            return TextProcessor.summarize_text(clean_results, 3)
        elif query_type == "wiki":
            return TextProcessor.summarize_text(clean_results, 3)
        else:
            try:
                processed_response = NeuralAssistant._process_with_neural(clean_results)
                if processed_response:
                    return processed_response
            except Exception as e:
                log_event(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é: {str(e)}")
            return TextProcessor.summarize_text(clean_results, 4)
@staticmethod
def _process_with_neural(text):
    try:
        sentences = re.split(r'(?<=[.!?])\s+', text)
        if len(sentences) <= 3:
            return text
        keywords = TextProcessor.extract_keywords(text, max_keywords=10)
        scored_sentences = []
        for sentence in sentences:
            score = 0
            for keyword in keywords:
                if keyword.lower() in sentence.lower():
                    score += 1
            scored_sentences.append((sentence, score))
        sorted_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)
        top_sentences = sorted_sentences[:3]
        ordered_sentences = [s[0] for s in top_sentences]
        result = " ".join(ordered_sentences)
        if len(result) < 100 and len(sorted_sentences) > 3:
            additional_sentences = [s[0] for s in sorted_sentences[3:5]]
            result += " " + " ".join(additional_sentences)
        return result
    except Exception as e:
        log_event(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {str(e)}")
        return None
class QueryClassifier:
    PATTERNS = {
        "weather": [
            r"–ø–æ–≥–æ–¥–∞ –≤ (\w+)",
            r"–∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ (\w+)",
            r"–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã (\w+)",
            r"—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ (\w+)",
            r"–ø–æ–≥–æ–¥–∞ (\w+)",
            r"–≤ (\w+) –ø–æ–≥–æ–¥–∞",
            r"–≤ (\w+) –∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞",
            r"–∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å –≤ (\w+)",
            r"–ø–æ–≥–æ–¥–∞ —Å–µ–π—á–∞—Å –≤ (\w+)",
            r"–ø–æ–≥–æ–¥–∞ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è –≤ (\w+)",
            r"–ø—Ä–æ–≥–Ω–æ–∑ –≤ (\w+)"
        ],
        "currency": [
            r"–∫—É—Ä—Å –≤–∞–ª—é—Ç",
            r"–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞",
            r"–∫—É—Ä—Å –µ–≤—Ä–æ",
            r"–∫—É—Ä—Å —é–∞–Ω—è",
            r"–≤–∞–ª—é—Ç–∞",
            r"–æ–±–º–µ–Ω –≤–∞–ª—é—Ç",
            r"–¥–µ–Ω—å–≥–∏",
            r"—Ä—É–±–ª—å",
            r"–¥–æ–ª–ª–∞—Ä",
            r"–µ–≤—Ä–æ",
            r"—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –¥–æ–ª–ª–∞—Ä",
            r"—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –µ–≤—Ä–æ",
            r"—Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–æ–ª–ª–∞—Ä–∞",
            r"—Å—Ç–æ–∏–º–æ—Å—Ç—å –µ–≤—Ä–æ"
        ],
        "news": [
            r"–Ω–æ–≤–æ—Å—Ç–∏",
            r"–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
            r"—á—Ç–æ –Ω–æ–≤–æ–≥–æ",
            r"—Å–æ–±—ã—Ç–∏—è",
            r"–Ω–æ–≤–æ—Å—Ç–∏ (\w+)",
            r"—á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç",
            r"—Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏",
            r"–≥–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏",
            r"–Ω–æ–≤–æ—Å—Ç–∏ –¥–Ω—è",
            r"–Ω–æ–≤–æ—Å—Ç–∏ —Å–µ–≥–æ–¥–Ω—è"
        ],
        "wiki": [
            r"–∫—Ç–æ —Ç–∞–∫–æ–π",
            r"—á—Ç–æ —Ç–∞–∫–æ–µ",
            r"–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
            r"–∑–Ω–∞—á–µ–Ω–∏–µ",
            r"–≤–∏–∫–∏–ø–µ–¥–∏—è",
            r"–æ–±—ä—è—Å–Ω–∏",
            r"—Ä–∞—Å—Å–∫–∞–∂–∏ –æ",
            r"–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ",
            r"–∏—Å—Ç–æ—Ä–∏—è",
            r"–±–∏–æ–≥—Ä–∞—Ñ–∏—è",
            r"–∫—Ç–æ —Ç–∞–∫–∞—è",
            r"–∫—Ç–æ —Ç–∞–∫–∏–µ",
            r"—á—Ç–æ —ç—Ç–æ —Ç–∞–∫–æ–µ",
            r"—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç"
        ],
        "greeting": [
            r"^–ø—Ä–∏–≤–µ—Ç$",
            r"^–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π$",
            r"^—Ö–∞–π$",
            r"^–∑–¥–æ—Ä–æ–≤–æ$",
            r"^hi$",
            r"^hello$",
            r"^–¥–æ–±—Ä—ã–π –¥–µ–Ω—å$",
            r"^–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ$",
            r"^–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä$"
        ]
    }
    @staticmethod
    def classify(query):
        query = query.lower().strip()
        for pattern in QueryClassifier.PATTERNS["greeting"]:
            if re.match(pattern, query):
                return "greeting", None
        if "–ø–æ–≥–æ–¥–∞" in query:
            for pattern in QueryClassifier.PATTERNS["weather"]:
                match = re.search(pattern, query)
                if match:
                    return "weather", match.group(1)
            for city, variants in NeuralAssistant.CITIES.items():
                for variant in variants:
                    if variant in query:
                        return "weather", city
            return "weather", None
        for pattern in QueryClassifier.PATTERNS["currency"]:
            if re.search(pattern, query):
                return "currency", None
        for pattern in QueryClassifier.PATTERNS["news"]:
            match = re.search(pattern, query)
            if match:
                if match.groups():
                    return "news", match.group(1)
                return "news", None
        for pattern in QueryClassifier.PATTERNS["wiki"]:
            if re.search(pattern, query):
                parts = query.split(" –æ ", 1)
                if len(parts) > 1:
                    return "wiki", parts[1]
                for prefix in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è", "—á—Ç–æ —ç—Ç–æ", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–∑–Ω–∞—á–µ–Ω–∏–µ"]:
                    if query.startswith(prefix):
                        return "wiki", query[len(prefix):].strip()
                return "wiki", query
        return "web", query
class SearchThread(QThread):
    result_ready = pyqtSignal(str, str)  
    error_occurred = pyqtSignal(str)  
    def __init__(self, query, search_type, original_query=""):
        super().__init__()
        self.query = query
        self.search_type = search_type
        self.original_query = original_query if original_query else query
        self.result = None  
    def run(self):
        try:
            corrected_info = ""
            if self.original_query.lower() != self.query.lower() and self.search_type != "conversation":
                corrected_info = f"–£—Ç–æ—á–Ω–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: '{self.query}'\n\n"
            if self.search_type == "weather":
                city = self.extract_city_from_query(self.query)
                if not city:
                    city = "–º–æ—Å–∫–≤–∞"
                    corrected_info += "–ì–æ—Ä–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω, –ø–æ–∫–∞–∑—ã–≤–∞—é –ø–æ–≥–æ–¥—É –≤ –ú–æ—Å–∫–≤–µ.\n\n"
                raw_result = self.get_weather(city)
                processed_result = NeuralAssistant.process_search_results(raw_result, "weather")
                self.result = corrected_info + processed_result
                self.result_ready.emit(corrected_info + processed_result, "–ü–æ–≥–æ–¥–∞")
            elif self.search_type == "currency":
                raw_result = self.get_currency()
                processed_result = NeuralAssistant.process_search_results(raw_result, "currency")
                self.result = corrected_info + processed_result
                self.result_ready.emit(corrected_info + processed_result, "–ö—É—Ä—Å—ã –≤–∞–ª—é—Ç")
            elif self.search_type == "wiki":
                raw_result = self.search_wikipedia(self.query)
                processed_result = NeuralAssistant.process_search_results(raw_result, "wiki")
                self.result = corrected_info + processed_result
                self.result_ready.emit(corrected_info + processed_result, "–í–∏–∫–∏–ø–µ–¥–∏—è")
            elif self.search_type == "news":
                raw_result = self.get_news(self.query)
                processed_result = NeuralAssistant.process_search_results(raw_result, "news")
                self.result = corrected_info + processed_result
                self.result_ready.emit(corrected_info + processed_result, "–ù–æ–≤–æ—Å—Ç–∏")
            elif self.search_type == "conversation":
                self.result = self.query
                self.result_ready.emit(self.query, "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç")
            else:
                raw_result = self.search_web(self.query)
                processed_result = NeuralAssistant.process_search_results(raw_result, "web")
                self.result = corrected_info + processed_result
                self.result_ready.emit(corrected_info + processed_result, "–í–µ–±-–ø–æ–∏—Å–∫")
        except Exception as e:
            self.error_occurred.emit(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}")
            log_event(f"–û—à–∏–±–∫–∞: {str(e)}")
    def extract_city_from_query(self, query):
        patterns = [
            r"–ø–æ–≥–æ–¥–∞ (?:–≤ )?(\w+)",
            r"(?:–≤ )?(\w+) –ø–æ–≥–æ–¥–∞",
            r"(?:–≤ )?(\w+) (?:–∫–∞–∫–∞—è )?–ø–æ–≥–æ–¥–∞",
            r"(?:–∫–∞–∫–∞—è )?–ø–æ–≥–æ–¥–∞ (?:–≤ )?(\w+)",
        ]
        for pattern in patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                return match.group(1).lower()
        for city, variants in NeuralAssistant.CITIES.items():
            for variant in variants:
                if variant in query.lower():
                    return city
        return None
    def handle_greeting(self):
        import random
        greetings = [
            "–ü—Ä–∏–≤–µ—Ç! –ß–µ–º —è –º–æ–≥—É –ø–æ–º–æ—á—å?",
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –ß—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ —É–∑–Ω–∞—Ç—å?",
            "–î–æ–±—Ä—ã–π –¥–µ–Ω—å! –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å, –∏ —è –Ω–∞–π–¥—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.",
            "–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é! –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å –ø–æ–∏—Å–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.",
            "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –Ø —É–º–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤–∏–∫. –ß—Ç–æ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç?"
        ]
        return random.choice(greetings)
    def search_web(self, query):
        try:
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=5))
            if not results:
                return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å."
            raw_results = ""
            for i, r in enumerate(results, 1):
                content = r.get('body', '').strip()
                if content:
                    raw_results += f"{content}\n\n"
            return raw_results
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ DuckDuckGo: {str(e)}")
            try:
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
                search_url = f"https://www.bing.com/search?q={query}"
                response = requests.get(search_url, headers=headers)
                soup = BeautifulSoup(response.text, 'html.parser')
                results = []
                for result in soup.select('.b_algo')[:5]:
                    title = result.select_one('h2')
                    snippet = result.select_one('.b_caption p')
                    if title and snippet:
                        results.append({
                            'title': title.get_text(),
                            'snippet': snippet.get_text()
                        })
                if not results:
                    return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å."
                raw_results = ""
                for i, r in enumerate(results, 1):
                    content = r.get('snippet', '').strip()
                    if content:
                        raw_results += f"{content}\n\n"
                return raw_results
            except Exception as e2:
                log_event(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∞—Å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e2)}")
                return "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É."
    def search_wikipedia(self, query):
        wikipedia.set_lang("ru")
        try:
            topic = query
            for prefix in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π", "–∫—Ç–æ —Ç–∞–∫–∞—è", "—á—Ç–æ —ç—Ç–æ", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–∑–Ω–∞—á–µ–Ω–∏–µ"]:
                if query.lower().startswith(prefix):
                    topic = query[len(prefix):].strip()
                    break
            search_results = wikipedia.search(topic, results=3)
            if not search_results:
                keywords = TextProcessor.extract_keywords(topic, max_keywords=2)
                if keywords:
                    search_results = wikipedia.search(keywords[0], results=3)
            if not search_results:
                web_result = self.search_web(f"{topic} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
                return f"–í –í–∏–∫–∏–ø–µ–¥–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–æ –≤–æ—Ç —á—Ç–æ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏:\n\n{web_result}"
            try:
                page = wikipedia.page(search_results[0], auto_suggest=False)
            except wikipedia.exceptions.DisambiguationError as e:
                if e.options:
                    page = wikipedia.page(e.options[0], auto_suggest=False)
                else:
                    raise
            summary = wikipedia.summary(page.title, sentences=5)
            if topic.lower() not in page.title.lower() and topic.lower() not in summary.lower()[:100]:
                result = f"üìö {page.title}\n\n–í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –Ω–µ —Å–æ–≤—Å–µ–º —Ç–æ, —á—Ç–æ –≤—ã –∏—Å–∫–∞–ª–∏, –Ω–æ –≤–æ—Ç —á—Ç–æ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏:\n\n{summary}"
            else:
                result = f"üìö {page.title}\n\n{summary}"
            return result
        except wikipedia.exceptions.DisambiguationError as e:
            options = e.options[:5]
            result = f"–£—Ç–æ—á–Ω–∏—Ç–µ –∑–∞–ø—Ä–æ—Å. –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:\n"
            for option in options:
                result += f"‚Ä¢ {option}\n"
            return result
        except wikipedia.exceptions.PageError:
            web_result = self.search_web(f"{topic} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
            return f"–í –í–∏–∫–∏–ø–µ–¥–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–æ –≤–æ—Ç —á—Ç–æ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏:\n\n{web_result}"
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –í–∏–∫–∏–ø–µ–¥–∏–∏: {str(e)}")
            web_result = self.search_web(f"{topic} –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
            return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –í–∏–∫–∏–ø–µ–¥–∏–∏, –Ω–æ –≤–æ—Ç —á—Ç–æ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏:\n\n{web_result}"
    def get_weather(self, location):
        try:
            api_key = "1085c9bb91ff9b12e324b7739fa6ec43"  
            url = f"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={api_key}&units=metric&lang=ru"
            response = requests.get(url)
            data = response.json()
            if response.status_code == 200:
                city = data["name"]
                country = data["sys"]["country"]
                temp = data["main"]["temp"]
                feels_like = data["main"]["feels_like"]
                description = data["weather"][0]["description"]
                humidity = data["main"]["humidity"]
                wind_speed = data["wind"]["speed"]
                weather_id = data["weather"][0]["id"]
                icon = "ÔøΩÔøΩÔ∏è"  
                if weather_id >= 200 and weather_id < 300:  
                    icon = "‚õàÔ∏è"
                elif weather_id >= 300 and weather_id < 400:  
                    icon = "üåßÔ∏è"
                elif weather_id >= 500 and weather_id < 600:  
                    icon = "üåßÔ∏è"
                elif weather_id >= 600 and weather_id < 700:  
                    icon = "‚ùÑÔ∏è"
                elif weather_id >= 700 and weather_id < 800:  
                    icon = "üå´Ô∏è"
                elif weather_id == 800:  
                    icon = "‚òÄÔ∏è"
                elif weather_id > 800:  
                    icon = "‚òÅÔ∏è"
                result = f"{icon} –ü–æ–≥–æ–¥–∞ –≤ {city}, {country}:\n\n"
                result += f"‚Ä¢ –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {temp:.1f}¬∞C (–æ—â—É—â–∞–µ—Ç—Å—è –∫–∞–∫ {feels_like:.1f}¬∞C)\n"
                result += f"‚Ä¢ –°–æ—Å—Ç–æ—è–Ω–∏–µ: {description}\n"
                result += f"‚Ä¢ –í–ª–∞–∂–Ω–æ—Å—Ç—å: {humidity}%\n"
                result += f"‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å –≤–µ—Ç—Ä–∞: {wind_speed} –º/—Å\n\n"
                try:
                    forecast_url = f"https://api.openweathermap.org/data/2.5/forecast?q={location}&appid={api_key}&units=metric&lang=ru&cnt=3"
                    forecast_response = requests.get(forecast_url)
                    forecast_data = forecast_response.json()
                    if forecast_response.status_code == 200:
                        result += "–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è:\n"
                        for item in forecast_data["list"][:2]:  
                            dt = datetime.datetime.fromtimestamp(item["dt"])
                            temp = item["main"]["temp"]
                            description = item["weather"][0]["description"]
                            result += f"‚Ä¢ {dt.strftime('%H:%M')}: {temp:.1f}¬∞C, {description}\n"
                except Exception as e:
                    log_event(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞: {str(e)}")
                result += f"\n–î–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {datetime.datetime.now().strftime('%H:%M:%S')}"
                return result
            else:
                log_event(f"–û—à–∏–±–∫–∞ API –ø–æ–≥–æ–¥—ã: {data.get('message', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                try:
                    backup_url = f"https://wttr.in/{location}?format=%l:+%c+%t+%w+%h"
                    backup_response = requests.get(backup_url)
                    if backup_response.status_code == 200 and len(backup_response.text) > 10:
                        weather_text = backup_response.text
                        return f"üå§Ô∏è {weather_text}\n\n–î–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {datetime.datetime.now().strftime('%H:%M:%S')}"
                except Exception as backup_error:
                    log_event(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∞—Å–Ω–æ–≥–æ API –ø–æ–≥–æ–¥—ã: {str(backup_error)}")
                return self.search_web(f"–ø–æ–≥–æ–¥–∞ {location} —Å–µ–π—á–∞—Å")
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–≥–æ–¥—ã: {str(e)}")
            return self.search_web(f"–ø–æ–≥–æ–¥–∞ {location} —Å–µ–π—á–∞—Å")
    def get_currency(self):
        try:
            url = "https://www.cbr-xml-daily.ru/daily_json.js"
            response = requests.get(url)
            data = response.json()
            if response.status_code == 200:
                usd = data["Valute"]["USD"]["Value"]
                eur = data["Valute"]["EUR"]["Value"]
                cny = data["Valute"]["CNY"]["Value"]
                usd_prev = data["Valute"]["USD"]["Previous"]
                eur_prev = data["Valute"]["EUR"]["Previous"]
                cny_prev = data["Valute"]["CNY"]["Previous"]
                usd_change = usd - usd_prev
                eur_change = eur - eur_prev
                cny_change = cny - cny_prev
                result = f"üí∞ –ö—É—Ä—Å—ã –≤–∞–ª—é—Ç (–¶–ë –†–§):\n\n"
                result += f"‚Ä¢ USD: {usd:.2f} ‚ÇΩ ({'+' if usd_change >= 0 else ''}{usd_change:.2f})\n"
                result += f"‚Ä¢ EUR: {eur:.2f} ‚ÇΩ ({'+' if eur_change >= 0 else ''}{eur_change:.2f})\n"
                result += f"‚Ä¢ CNY: {cny:.2f} ‚ÇΩ ({'+' if cny_change >= 0 else ''}{cny_change:.2f})\n\n"
                result += f"–î–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {data['Date'][:10]}"
                return result
            else:
                return self.search_web("–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –µ–≤—Ä–æ —é–∞–Ω—è —Å–µ–≥–æ–¥–Ω—è")
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç: {str(e)}")
            return self.search_web("–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞ –µ–≤—Ä–æ —é–∞–Ω—è —Å–µ–≥–æ–¥–Ω—è")
    def get_news(self, query=""):
        try:
            topic = None
            for word in query.split():
                if word.lower() not in ["–Ω–æ–≤–æ—Å—Ç–∏", "–Ω–æ–≤–æ—Å—Ç—å", "–ø–æ—Å–ª–µ–¥–Ω–∏–µ", "—Å–≤–µ–∂–∏–µ", "—Å–æ–±—ã—Ç–∏—è"]:
                    topic = word
                    break
            url = "https://newsdata.io/api/1/news"
            params = {
                "apikey": "pub_3048983f5c4f1a4b1df8d1d5c3e8a7a5e1c0c",  
                "language": "ru",
                "q": topic if topic else None
            }
            response = requests.get(url, params={k: v for k, v in params.items() if v is not None})
            data = response.json()
            if response.status_code == 200 and data.get("status") == "success":
                articles = data.get("results", [])
                if not articles:
                    return self.search_web(f"–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ {topic if topic else ''}")
                result = f"üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏{' –ø–æ –∑–∞–ø—Ä–æ—Å—É: ' + topic if topic else ''}:\n\n"
                for i, article in enumerate(articles[:5], 1):
                    title = article.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")
                    description = article.get("description", "")
                    if not description:
                        description = article.get("content", "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è")
                    source = article.get("source_id", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫")
                    title = TextProcessor.clean_search_results(title)
                    description = TextProcessor.clean_search_results(description)
                    result += f"{title}\n"
                    if description:
                        first_sentence = description.split('.')[0]
                        if len(first_sentence) > 10:  
                            result += f"{first_sentence}.\n"
                    result += f"–ò—Å—Ç–æ—á–Ω–∏–∫: {source}\n\n"
                return result
            else:
                error_msg = data.get("message", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞")
                log_event(f"–û—à–∏–±–∫–∞ API –Ω–æ–≤–æ—Å—Ç–µ–π: {error_msg}")
                return self.search_web(f"–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ {topic if topic else ''}")
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e)}")
            try:
                url = "https://news.yandex.ru/ru/index5.rss"
                response = requests.get(url)
                soup = BeautifulSoup(response.text, 'xml')
                items = soup.find_all('item')
                if not items:
                    return "–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
                result = f"üì∞ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:\n\n"
                for i, item in enumerate(items[:5], 1):
                    title = item.find('title').text
                    description = item.find('description').text
                    title = TextProcessor.clean_search_results(title)
                    description = TextProcessor.clean_search_results(description)
                    result += f"{title}\n"
                    first_sentence = description.split('.')[0]
                    if len(first_sentence) > 10:
                        result += f"{first_sentence}.\n\n"
                    else:
                        result += "\n"
                return result
            except Exception as e2:
                log_event(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∞—Å–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π: {str(e2)}")
                return self.search_web(f"–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ {topic if topic else ''}")
class SmartSearchApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é")
        self.resize(800, 600)
        self.search_thread = None
        self.neural_thread = None
        self.speech_thread = None
        self.thinking_timer = None
        self.thinking_dots = 0
        self.chat_history = []
        self.dialog_context = DialogContext()  
        self.nlp = NaturalLanguageProcessor()  
        self.setup_dark_theme()
        self.setup_ui()
        log_event("–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ")
    def setup_dark_theme(self):
        app = QApplication.instance()
        dark_palette = QPalette()
        dark_palette.setColor(QPalette.ColorRole.Window, QColor(33, 33, 33))
        dark_palette.setColor(QPalette.ColorRole.WindowText, QColor(255, 255, 255))
        dark_palette.setColor(QPalette.ColorRole.Base, QColor(25, 25, 25))
        dark_palette.setColor(QPalette.ColorRole.AlternateBase, QColor(45, 45, 45))
        dark_palette.setColor(QPalette.ColorRole.ToolTipBase, QColor(25, 25, 25))
        dark_palette.setColor(QPalette.ColorRole.ToolTipText, QColor(255, 255, 255))
        dark_palette.setColor(QPalette.ColorRole.Text, QColor(255, 255, 255))
        dark_palette.setColor(QPalette.ColorRole.Button, QColor(45, 45, 45))
        dark_palette.setColor(QPalette.ColorRole.ButtonText, QColor(255, 255, 255))
        dark_palette.setColor(QPalette.ColorRole.Link, QColor(42, 130, 218))
        dark_palette.setColor(QPalette.ColorRole.Highlight, QColor(42, 130, 218))
        dark_palette.setColor(QPalette.ColorRole.HighlightedText, QColor(0, 0, 0))
        app.setPalette(dark_palette)
        app.setStyleSheet("""
            QToolTip { 
                color: #ffffff; 
                background-color: #2a82da; 
                border: 1px solid white; 
            }
            QWidget {
                background-color: #212121;
                color: #ffffff;
            }
            QTextEdit, QLineEdit { 
                background-color: #1a1a1a; 
                color: #ffffff; 
                border: 1px solid #3a3a3a;
                border-radius: 5px;
                padding: 5px;
            }
            QPushButton { 
                background-color: #2a82da; 
                color: white; 
                border: none;
                border-radius: 5px;
                padding: 8px 16px;
            }
            QPushButton:hover { 
                background-color: #3a92ea; 
            }
            QPushButton:pressed { 
                background-color: #1a72ca; 
            }
            QComboBox {
                background-color: #1a1a1a;
                color: #ffffff;
                border: 1px solid #3a3a3a;
                border-radius: 5px;
                padding: 5px;
            }
            QComboBox QAbstractItemView {
                background-color: #1a1a1a;
                color: #ffffff;
                selection-background-color: #2a82da;
            }
            QProgressBar {
                border: 1px solid #3a3a3a;
                border-radius: 5px;
                background-color: #1a1a1a;
                text-align: center;
                color: white;
            }
            QProgressBar::chunk {
                background-color: #2a82da;
                width: 10px;
                margin: 0.5px;
            }
        """)
    def setup_ui(self):
        main_layout = QVBoxLayout()
        title_label = QLabel("üß† –£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é")
        title_font = QFont()
        title_font.setPointSize(16)
        title_font.setBold(True)
        title_label.setFont(title_font)
        title_label.setAlignment(Qt.AlignmentFlag.AlignCenter)
        main_layout.addWidget(title_label)
        self.chat_area = QTextEdit()
        self.chat_area.setReadOnly(True)
        self.chat_area.setStyleSheet("""
            QTextEdit {
                background-color: #2d2d2d;
                color: #e0e0e0;
                border: 1px solid #444444;
                border-radius: 5px;
                padding: 10px;
            }
        """)
        main_layout.addWidget(self.chat_area)
        bottom_panel = QHBoxLayout()
        self.input_field = QLineEdit()
        self.input_field.setPlaceholderText("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å...")
        self.input_field.returnPressed.connect(self.send_query)
        self.input_field.setStyleSheet("""
            QLineEdit {
                background-color: #3d3d3d;
                color: #ffffff;
                border: 1px solid #555555;
                border-radius: 5px;
                padding: 8px;
                font-size: 14px;
            }
        """)
        bottom_panel.addWidget(self.input_field)
        self.voice_button = QToolButton()
        self.voice_button.setText("üé§")
        self.voice_button.setToolTip("–ì–æ–ª–æ—Å–æ–≤–æ–π –≤–≤–æ–¥")
        self.voice_button.setStyleSheet("""
            QToolButton {
                background-color: #3d3d3d;
                color: #ffffff;
                border: 1px solid #555555;
                border-radius: 5px;
                padding: 8px;
                font-size: 16px;
            }
            QToolButton:hover {
                background-color: #4a4a4a;
            }
            QToolButton:pressed {
                background-color: #2a82da;
            }
        """)
        self.voice_button.clicked.connect(self.start_voice_input)
        bottom_panel.addWidget(self.voice_button)
        self.send_button = QPushButton("–ù–∞–π—Ç–∏")
        self.send_button.clicked.connect(self.send_query)
        self.send_button.setStyleSheet("""
            QPushButton {
                background-color: #2a82da;
                color: #ffffff;
                border: none;
                border-radius: 5px;
                padding: 8px 15px;
                font-size: 14px;
            }
            QPushButton:hover {
                background-color: #3a92ea;
            }
            QPushButton:pressed {
                background-color: #1a72ca;
            }
        """)
        bottom_panel.addWidget(self.send_button)
        main_layout.addLayout(bottom_panel)
        options_panel = QHBoxLayout()
        self.assistant_mode = QCheckBox("–†–µ–∂–∏–º –ø–æ–º–æ—â–Ω–∏–∫–∞")
        self.assistant_mode.setToolTip("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ç–µ–º–∞–º–∏")
        self.assistant_mode.setChecked(True)
        self.assistant_mode.setStyleSheet("""
            QCheckBox {
                color: #e0e0e0;
            }
            QCheckBox::indicator:checked {
                background-color: #2a82da;
                border: 1px solid #2a82da;
            }
        """)
        options_panel.addWidget(self.assistant_mode)
        self.dialog_mode = QCheckBox("–î–∏–∞–ª–æ–≥–æ–≤—ã–π —Ä–µ–∂–∏–º")
        self.dialog_mode.setToolTip("–ó–∞–ø–æ–º–∏–Ω–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞")
        self.dialog_mode.setChecked(True)
        self.dialog_mode.setStyleSheet("""
            QCheckBox {
                color: #e0e0e0;
            }
            QCheckBox::indicator:checked {
                background-color: #2a82da;
                border: 1px solid #2a82da;
            }
        """)
        options_panel.addWidget(self.dialog_mode)
        options_panel.addStretch()
        main_layout.addLayout(options_panel)
        self.progress_bar = QProgressBar()
        self.progress_bar.setRange(0, 0)  
        self.progress_bar.setVisible(False)
        main_layout.addWidget(self.progress_bar)
        self.status_label = QLabel("–ì–æ—Ç–æ–≤ –∫ –ø–æ–∏—Å–∫—É")
        self.status_label.setAlignment(Qt.AlignmentFlag.AlignRight)
        main_layout.addWidget(self.status_label)
        self.setLayout(main_layout)
        welcome_msg = """
        <h3 style="color: #2a82da;">üëã –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é!</h3>
        <p>–Ø –º–æ–≥—É –ø–æ–º–æ—á—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø–æ–¥–¥–µ—Ä–∂–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä.</p>
        <p>–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:</p>
        <ul>
            <li>–ü–æ–≥–æ–¥–∞ –≤ –ú–æ—Å–∫–≤–µ</li>
            <li>–ö—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞</li>
            <li>–ü–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏</li>
            <li>–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å</li>
            <li>–•–æ—á—É —É–µ—Ö–∞—Ç—å –≤ –æ—Ç–ø—É—Å–∫ –≤ –¢—É—Ä—Ü–∏—é, –∫–∞–∫–∞—è —Ç–∞–º –ø–æ–≥–æ–¥–∞ –∏ –∫—É—Ä—Å –≤–∞–ª—é—Ç?</li>
        </ul>
        <p>–ü—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞.</p>
        """
        self.append_message("–£–º–Ω—ã–π –ø–æ–∏—Å–∫", welcome_msg)
    def send_query(self):
        query = self.input_field.text().strip()
        if not query:
            return
        self.append_message("–í—ã", query)
        self.input_field.clear()
        self.start_thinking_animation()
        self.status_label.setText("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞...")
        self.send_button.setEnabled(False)
        self.progress_bar.setVisible(True)
        self.neural_thread = NeuralThread(query, self.dialog_mode.isChecked(), self.dialog_context)
        self.neural_thread.result_ready.connect(self.handle_neural_result)
        self.neural_thread.error_occurred.connect(self.handle_search_error)
        self.neural_thread.start()
    def start_voice_input(self):
        if self.speech_thread and self.speech_thread.is_listening:
            return
        self.voice_button.setEnabled(False)
        self.voice_button.setText("üîä")
        self.status_label.setText("–°–ª—É—à–∞—é...")
        self.speech_thread = SpeechRecognitionThread()
        self.speech_thread.text_recognized.connect(self.handle_recognized_text)
        self.speech_thread.error_occurred.connect(self.handle_speech_error)
        self.speech_thread.status_update.connect(self.update_speech_status)
        self.speech_thread.start()
    def handle_recognized_text(self, text):
        self.voice_button.setEnabled(True)
        self.voice_button.setText("üé§")
        self.input_field.setText(text)
        self.status_label.setText("–¢–µ–∫—Å—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω")
        self.send_query()
    def handle_speech_error(self, error_msg):
        self.voice_button.setEnabled(True)
        self.voice_button.setText("üé§")
        self.status_label.setText(error_msg)
        log_event(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏: {error_msg}")
    def update_speech_status(self, status):
        self.status_label.setText(status)
        if status == "–ì–æ—Ç–æ–≤ –∫ –ø–æ–∏—Å–∫—É":
            self.voice_button.setEnabled(True)
            self.voice_button.setText("üé§")
    def handle_neural_result(self, query_type, processed_query, category, original_query):
        log_event(f"–ó–∞–ø—Ä–æ—Å: {original_query} -> {query_type}/{category} -> {processed_query}")
        if self.search_thread and self.search_thread.isRunning():
            self.search_thread.terminate()
            self.search_thread.wait()
        if query_type == "conversation":
            self.append_message("–£–º–Ω—ã–π –ø–æ–∏—Å–∫ (–ù–µ–π—Ä–æ—Å–µ—Ç—å)", processed_query)
            if self.dialog_mode.isChecked():
                self.dialog_context.add_interaction(original_query, processed_query, "conversation")
            self.search_finished()
        elif query_type == "complex" and self.assistant_mode.isChecked():
            self.status_label.setText("–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–∂–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞...")
            complex_thread = threading.Thread(
                target=self.process_complex_scenario,
                args=(processed_query, original_query)
            )
            complex_thread.daemon = True
            complex_thread.start()
        else:
            self.search_thread = SearchThread(processed_query, category, original_query)
            self.search_thread.result_ready.connect(self.handle_search_result)
            self.search_thread.error_occurred.connect(self.handle_search_error)
            self.search_thread.start()
    def process_complex_scenario(self, subtopics, original_query):
        try:
            result = NeuralAssistant.process_complex_scenario(subtopics)
            QApplication.instance().postEvent(
                self,
                QTimer(self, singleShot=True, timeout=lambda: self.handle_complex_result(result, original_query))
            )
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–∂–Ω–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è: {str(e)}")
            QApplication.instance().postEvent(
                self,
                QTimer(self, singleShot=True, timeout=lambda: self.handle_search_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å–ª–æ–∂–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"))
            )
    def handle_complex_result(self, result, original_query):
        self.append_message("–£–º–Ω—ã–π –ø–æ–∏—Å–∫ (–ü–æ–º–æ—â–Ω–∏–∫)", result)
        if self.dialog_mode.isChecked():
            self.dialog_context.add_interaction(original_query, result, "complex")
        self.search_finished()
    def start_thinking_animation(self):
        if not self.thinking_timer:
            self.thinking_timer = QTimer(self)
            self.thinking_timer.timeout.connect(self.update_thinking_animation)
        self.thinking_dots = 0
        self.status_label.setText("–î—É–º–∞—é")
        self.thinking_timer.start(300)  
    def update_thinking_animation(self):
        self.thinking_dots = (self.thinking_dots + 1) % 4
        dots = "." * self.thinking_dots
        self.status_label.setText(f"–î—É–º–∞—é{dots}")
    def stop_thinking_animation(self):
        if self.thinking_timer and self.thinking_timer.isActive():
            self.thinking_timer.stop()
    def handle_search_result(self, result, source):
        if not result.strip():
            result = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
        self.append_message(f"–£–º–Ω—ã–π –ø–æ–∏—Å–∫ ({source})", result)
        log_event(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç {source}: {result[:100]}...")
        if self.dialog_mode.isChecked() and self.search_thread:
            query_type = self.search_thread.search_type
            entities = {}
            if query_type == "weather":
                city = self.search_thread.extract_city_from_query(self.search_thread.query)
                if city:
                    entities["city"] = city
            elif query_type == "news":
                for word in self.search_thread.query.split():
                    if word.lower() not in ["–Ω–æ–≤–æ—Å—Ç–∏", "–Ω–æ–≤–æ—Å—Ç—å", "–ø–æ—Å–ª–µ–¥–Ω–∏–µ", "—Å–≤–µ–∂–∏–µ", "—Å–æ–±—ã—Ç–∏—è"]:
                        entities["news_topic"] = word
                        break
            self.dialog_context.add_interaction(
                self.search_thread.original_query, 
                result, 
                query_type, 
                entities
            )
        self.search_finished()
    def handle_search_error(self, error_msg):
        self.append_message("–û—à–∏–±–∫–∞", error_msg)
        log_event(error_msg)
        self.search_finished()
    def search_finished(self):
        self.stop_thinking_animation()
        self.status_label.setText("–ì–æ—Ç–æ–≤ –∫ –ø–æ–∏—Å–∫—É")
        self.send_button.setEnabled(True)
        self.progress_bar.setVisible(False)
    def append_message(self, sender, message):
        current_time = datetime.datetime.now().strftime("%H:%M")
        if sender == "–í—ã":
            self.chat_area.append(f"<div style='background-color: #3a3a3a; padding: 8px; border-radius: 10px; margin: 5px;'>"
                                f"<b style='color: #ffffff;'>{sender}</b> <small style='color: #aaaaaa;'>({current_time})</small>:<br>{message}</div>")
        else:
            self.chat_area.append(f"<div style='background-color: #2d2d2d; padding: 8px; border-radius: 10px; margin: 5px;'>"
                                f"<b style='color: #4CAF50;'>{sender}</b> <small style='color: #aaaaaa;'>({current_time})</small>:<br>{message}</div>")
        scrollbar = self.chat_area.verticalScrollBar()
        scrollbar.setValue(scrollbar.maximum())
        self.chat_history.append({"sender": sender, "message": message, "time": current_time})
class NeuralThread(QThread):
    result_ready = pyqtSignal(str, str, str, str)  
    error_occurred = pyqtSignal(str)  
    def __init__(self, query, dialog_mode, dialog_context):
        super().__init__()
        self.query = query
        self.dialog_mode = dialog_mode
        self.dialog_context = dialog_context
    def run(self):
        try:
            query_type, processed_query, category = NeuralAssistant.process_query(self.query, self.dialog_context)
            self.result_ready.emit(query_type, processed_query, category, self.query)
        except Exception as e:
            self.error_occurred.emit(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
            log_event(f"–û—à–∏–±–∫–∞: {str(e)}")
class SpeechRecognitionThread(QThread):
    text_recognized = pyqtSignal(str)
    error_occurred = pyqtSignal(str)
    status_update = pyqtSignal(str)
    def __init__(self, language='ru-RU'):
        super().__init__()
        self.language = language
        self.recognizer = sr.Recognizer()
        self.is_listening = False
    def run(self):
        try:
            self.is_listening = True
            self.status_update.emit("–°–ª—É—à–∞—é...")
            with sr.Microphone() as source:
                self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                self.status_update.emit("–ì–æ–≤–æ—Ä–∏—Ç–µ...")
                audio = self.recognizer.listen(source, timeout=5)
                self.status_update.emit("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...")
                text = self.recognizer.recognize_google(audio, language=self.language)
                self.text_recognized.emit(text)
        except sr.WaitTimeoutError:
            self.error_occurred.emit("–í—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –∏—Å—Ç–µ–∫–ª–æ. –ù–µ —É—Å–ª—ã—à–∞–ª —Ä–µ—á–∏.")
        except sr.UnknownValueError:
            self.error_occurred.emit("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å.")
        except sr.RequestError as e:
            self.error_occurred.emit(f"–û—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")
        except Exception as e:
            self.error_occurred.emit(f"–û—à–∏–±–∫–∞: {str(e)}")
        finally:
            self.is_listening = False
            self.status_update.emit("–ì–æ—Ç–æ–≤ –∫ –ø–æ–∏—Å–∫—É")
class NaturalLanguageProcessor:
    def __init__(self):
        self.initialized = False
        self.classifier = None
        self.sentence_model = None
        self.summarizer = None
        self.query_reformulator = None
        self.initialization_thread = threading.Thread(target=self._initialize_models)
        self.initialization_thread.daemon = True
        self.initialization_thread.start()
    def _initialize_models(self):
        try:
            log_event("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –º–æ–¥–µ–ª–µ–π...")
            self.classifier = Pipeline([
                ('tfidf', TfidfVectorizer(max_features=5000)),
                ('clf', MultinomialNB())
            ])
            X_train = [
                "–ø–æ–≥–æ–¥–∞ –≤ –º–æ—Å–∫–≤–µ", "–∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ —Å–µ–≥–æ–¥–Ω—è", "–ø–æ–≥–æ–¥–∞ –Ω–∞ –∑–∞–≤—Ç—Ä–∞", "–¥–æ–∂–¥—å —Å–µ–≥–æ–¥–Ω—è", 
                "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≤ –ø–∏—Ç–µ—Ä–µ", "–ø—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã", "–±—É–¥–µ—Ç –ª–∏ —Å–Ω–µ–≥",
                "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–∫—É—Ä—Å –µ–≤—Ä–æ", "–≤–∞–ª—é—Ç–Ω—ã–π –∫—É—Ä—Å", "–æ–±–º–µ–Ω –≤–∞–ª—é—Ç—ã", "—Å—Ç–æ–∏–º–æ—Å—Ç—å –¥–æ–ª–ª–∞—Ä–∞",
                "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏", "—á—Ç–æ –Ω–æ–≤–æ–≥–æ", "–Ω–æ–≤–æ—Å—Ç–∏ –ø–æ–ª–∏—Ç–∏–∫–∏", "–Ω–æ–≤–æ—Å—Ç–∏ —Å–ø–æ—Ä—Ç–∞", 
                "–∫—Ç–æ —Ç–∞–∫–æ–π –ø—É—à–∫–∏–Ω", "—á—Ç–æ —Ç–∞–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞—è —Ñ–∏–∑–∏–∫–∞", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤–∞", "–∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞",
                "–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–∫–∞–∫ –¥–µ–ª–∞", "–¥–æ–±—Ä–æ–µ —É—Ç—Ä–æ", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å",
                "—Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä—é", "–ø–æ–∫–∞", "–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è"
            ]
            y_train = (
                ["weather"] * 7 + 
                ["currency"] * 5 + 
                ["news"] * 4 + 
                ["wiki"] * 4 + 
                ["greeting"] * 5 + 
                ["thanks_bye"] * 4
            )
            self.classifier.fit(X_train, y_train)
            try:
                self.sentence_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            except Exception as e:
                log_event(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ sentence-transformer: {str(e)}")
                self.sentence_model = None
            try:
                self.summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
            except Exception as e:
                log_event(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ summarizer: {str(e)}")
                self.summarizer = None
            try:
                self.query_reformulator = pipeline("text2text-generation", model="facebook/bart-base")
            except Exception as e:
                log_event(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ query_reformulator: {str(e)}")
                self.query_reformulator = None
            self.initialized = True
            log_event("NLP –º–æ–¥–µ–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ NLP –º–æ–¥–µ–ª–µ–π: {str(e)}")
    def classify_query(self, query):
        if not self.initialized or self.classifier is None:
            log_event("NLP –º–æ–¥–µ–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é")
            return self._basic_classify(query)
        try:
            predicted_class = self.classifier.predict([query])[0]
            return predicted_class
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
            return self._basic_classify(query)
    def _basic_classify(self, query):
        query = query.lower()
        if any(word in query for word in ["–ø–æ–≥–æ–¥–∞", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥"]):
            return "weather"
        elif any(word in query for word in ["–∫—É—Ä—Å", "–¥–æ–ª–ª–∞—Ä", "–µ–≤—Ä–æ", "–≤–∞–ª—é—Ç"]):
            return "currency"
        elif any(word in query for word in ["–Ω–æ–≤–æ—Å—Ç–∏", "–Ω–æ–≤–æ—Å—Ç—å", "—Å–æ–±—ã—Ç–∏—è"]):
            return "news"
        elif any(phrase in query for phrase in ["—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫—Ç–æ —Ç–∞–∫–æ–π", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–∑–Ω–∞—á–µ–Ω–∏–µ"]):
            return "wiki"
        elif any(word in query for word in ["–ø—Ä–∏–≤–µ—Ç", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "—Ö–∞–π", "–¥–æ–±—Ä—ã–π"]):
            return "greeting"
        elif any(word in query for word in ["—Å–ø–∞—Å–∏–±–æ", "–±–ª–∞–≥–æ–¥–∞—Ä—é", "–ø–æ–∫–∞", "—Å–≤–∏–¥–∞–Ω–∏—è"]):
            return "thanks_bye"
        else:
            return "web"
    def reformulate_query(self, query):
        if not self.initialized or self.query_reformulator is None:
            return query
        try:
            prompt = f"–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: '{query}'"
            result = self.query_reformulator(prompt, max_length=100, num_return_sequences=1)
            reformulated = result[0]['generated_text'].strip()
            if len(reformulated) < 5 or len(reformulated) > len(query) * 3:
                return query
            return reformulated
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
            return query
    def summarize_text(self, text, max_length=150):
        if not self.initialized or self.summarizer is None or len(text) < 200:
            return TextProcessor.summarize_text(text)
        try:
            max_input_length = 1024
            if len(text) > max_input_length:
                text = text[:max_input_length]
            summary = self.summarizer(text, max_length=max_length, min_length=30, do_sample=False)
            return summary[0]['summary_text']
        except Exception as e:
            log_event(f"–û—à–∏–±–∫–∞ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞: {str(e)}")
            return TextProcessor.summarize_text(text)
    def is_complex_scenario(self, query):
        topics = []
        if any(word in query.lower() for word in ["–ø–æ–≥–æ–¥–∞", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥"]):
            topics.append("weather")
        if any(word in query.lower() for word in ["–∫—É—Ä—Å", "–¥–æ–ª–ª–∞—Ä", "–µ–≤—Ä–æ", "–≤–∞–ª—é—Ç", "—Å—Ç–æ–∏—Ç"]):
            topics.append("currency")
        if any(word in query.lower() for word in ["–Ω–æ–≤–æ—Å—Ç–∏", "–Ω–æ–≤–æ—Å—Ç—å", "—Å–æ–±—ã—Ç–∏—è", "–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç"]):
            topics.append("news")
        return len(topics) > 1, topics
    def extract_subtopics(self, query):
        subtopics = {}
        if any(word in query.lower() for word in ["–ø–æ–≥–æ–¥–∞", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "–¥–æ–∂–¥—å", "—Å–Ω–µ–≥"]):
            city = None
            for pattern in NeuralAssistant.QUERY_PATTERNS["weather"]:
                match = re.search(pattern, query.lower())
                if match and match.groups():
                    city = match.group(1)
                    break
            if not city:
                for city_name, variants in NeuralAssistant.CITIES.items():
                    for variant in variants:
                        if variant in query.lower():
                            city = city_name
                            break
                    if city:
                        break
            if not city:
                countries = ["—Ç—É—Ä—Ü–∏—è", "—Ç—É—Ä—Ü–∏–∏", "–µ–≥–∏–ø–µ—Ç", "–µ–≥–∏–ø—Ç–µ", "–∏—Ç–∞–ª–∏—è", "–∏—Ç–∞–ª–∏–∏", "—Ñ—Ä–∞–Ω—Ü–∏—è", "—Ñ—Ä–∞–Ω—Ü–∏–∏", 
                           "–∏—Å–ø–∞–Ω–∏—è", "–∏—Å–ø–∞–Ω–∏–∏", "–≥—Ä–µ—Ü–∏—è", "–≥—Ä–µ—Ü–∏–∏", "—Ç–∞–∏–ª–∞–Ω–¥", "—Ç–∞–∏–ª–∞–Ω–¥–µ"]
                for country in countries:
                    if country in query.lower():
                        city = country
                        break
            if city:
                subtopics["weather"] = f"–ø–æ–≥–æ–¥–∞ –≤ {city}"
            else:
                subtopics["weather"] = "–ø–æ–≥–æ–¥–∞"
        if any(word in query.lower() for word in ["–∫—É—Ä—Å", "–¥–æ–ª–ª–∞—Ä", "–µ–≤—Ä–æ", "–≤–∞–ª—é—Ç", "—Å—Ç–æ–∏—Ç"]):
            subtopics["currency"] = "–∫—É—Ä—Å –≤–∞–ª—é—Ç"
        if any(word in query.lower() for word in ["–Ω–æ–≤–æ—Å—Ç–∏", "–Ω–æ–≤–æ—Å—Ç—å", "—Å–æ–±—ã—Ç–∏—è", "–ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç"]):
            news_topics = ["—Ä–µ–π—Å—ã", "–ø–æ–ª–µ—Ç—ã", "–∞–≤–∏–∞", "—Ç—É—Ä–∏–∑–º", "–æ—Ç–ø—É—Å–∫", "–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è", "–æ—Ç–¥—ã—Ö"]
            for topic in news_topics:
                if topic in query.lower():
                    subtopics["news"] = f"–Ω–æ–≤–æ—Å—Ç–∏ {topic}"
                    break
            if "news" not in subtopics:
                subtopics["news"] = "–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏"
        return subtopics
class DialogContext:
    def __init__(self, max_history=5):
        self.max_history = max_history
        self.history = []
        self.current_topic = None
        self.last_query_type = None
        self.entities = {}  
    def add_interaction(self, user_query, system_response, query_type=None, entities=None):
        interaction = {
            "user_query": user_query,
            "system_response": system_response,
            "timestamp": datetime.datetime.now(),
            "query_type": query_type
        }
        self.history.append(interaction)
        if len(self.history) > self.max_history:
            self.history = self.history[-self.max_history:]
        self.last_query_type = query_type
        if query_type in ["weather", "currency", "news", "wiki"]:
            self.current_topic = query_type
        if entities:
            self.entities.update(entities)
    def get_last_n_interactions(self, n=2):
        return self.history[-n:] if len(self.history) >= n else self.history
    def get_context_for_query(self, query):
        context = {
            "current_topic": self.current_topic,
            "last_query_type": self.last_query_type,
            "entities": self.entities,
            "recent_history": self.get_last_n_interactions(2)
        }
        return context
    def resolve_references(self, query):
        query_lower = query.lower()
        has_reference = any(word in query_lower for word in [
            "—Ç–∞–º", "—Ç—É–¥–∞", "—ç—Ç–æ—Ç", "—ç—Ç–∞", "—ç—Ç–æ", "—ç—Ç–∏", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ", "–æ–Ω–∏", 
            "–µ–≥–æ", "–µ–µ", "–∏—Ö", "—ç—Ç–æ–º", "—ç—Ç–æ–π", "—Ç–æ—Ç", "—Ç–∞", "—Ç–µ"
        ])
        if not has_reference or not self.history:
            return query
        if len(query_lower.split()) <= 2:
            if self.current_topic == "weather" and "city" in self.entities:
                if "–ø–æ–≥–æ–¥–∞" not in query_lower:
                    return f"–ø–æ–≥–æ–¥–∞ –≤ {self.entities['city']}"
            elif self.current_topic == "news" and "news_topic" in self.entities:
                if "–Ω–æ–≤–æ—Å—Ç–∏" not in query_lower:
                    return f"–Ω–æ–≤–æ—Å—Ç–∏ {self.entities['news_topic']}"
        last_interaction = self.history[-1] if self.history else None
        if last_interaction:
            last_query = last_interaction["user_query"].lower()
            for pattern in NeuralAssistant.QUERY_PATTERNS["weather"]:
                match = re.search(pattern, last_query)
                if match and match.groups():
                    city = match.group(1)
                    if "–ø–æ–≥–æ–¥–∞" in query_lower and not any(city_name in query_lower for city_name in NeuralAssistant.CITIES.keys()):
                        return f"–ø–æ–≥–æ–¥–∞ –≤ {city}"
        return query
    def should_continue_topic(self, query):
        if not self.current_topic or not self.history:
            return False
        query_lower = query.lower()
        has_reference = any(word in query_lower for word in [
            "—Ç–∞–º", "—Ç—É–¥–∞", "—ç—Ç–æ—Ç", "—ç—Ç–∞", "—ç—Ç–æ", "—ç—Ç–∏", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ", "–æ–Ω–∏", 
            "–µ–≥–æ", "–µ–µ", "–∏—Ö", "—ç—Ç–æ–º", "—ç—Ç–æ–π", "—Ç–æ—Ç", "—Ç–∞", "—Ç–µ", "–µ—â–µ", "–±–æ–ª—å—à–µ", 
            "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "–¥–µ—Ç–∞–ª—å–Ω–µ–µ"
        ])
        if len(query_lower.split()) <= 2:
            return True
        return has_reference
if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = SmartSearchApp()
    window.show()
    sys.exit(app.exec())